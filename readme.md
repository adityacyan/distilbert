# DistilBERT

A lightweight, fast, and efficient implementation of the DistilBERT model for natural language processing tasks.

## Features

- Distilled version of BERT for faster inference and smaller model size
- Supports various NLP tasks (classification, question answering, etc.)
- Easy integration and fine-tuning

## Installation

```bash
pip install -r requirements.txt

```

## Usage

```bash
python app.py

```



## Resources

- [Original DistilBERT Paper](https://arxiv.org/abs/1910.01108)
- [Hugging Face DistilBERT](https://huggingface.co/distilbert-base-uncased)

## License

This project is licensed under the MIT License.